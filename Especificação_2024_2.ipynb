{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alekswheeler/datasci_home_work/blob/main/Especifica%C3%A7%C3%A3o_2024_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFTJh1VJhXQO"
      },
      "source": [
        "# Trabalho Prático"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okgk7zc4h-OH"
      },
      "source": [
        "O objetivo geral deste trabalho é praticar os conceitos discutidos em sala de aula, principalmente: representação e pré-processamento de dados textuais; redução de dimensionalidade e algoritmos de agrupamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFEpZjjhcqH"
      },
      "source": [
        "## Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ezm41sjRx9"
      },
      "source": [
        "Para esse trabalho utilizaremos um conjunto de dados de filmes. Os dados foram obtidos de várias fontes, incluindo IMDB.\n",
        "\n",
        "O conjunto de dados que vocês deverão usar encontra-se no AVA da disciplina. Os dados estão organizados um arquivo (`.csv`) com as seguintes colunas:\n",
        "\n",
        "* **genres** - gêneros a que um filme pertence. Veja que um filme pode estar associado a mais de um gênero (`str`);\n",
        "* **sinopse** - sinopse do filme (`str`);\n",
        "* **startYear** - ano de lançamento do filme (`int`);\n",
        "* **primaryTitle** - título do filme (`str`);\n",
        "* **runtimeMinutes** - duração do filme, em minutos (`int`);\n",
        "* **averageRating** - média das avaliações do filme (`float`);\n",
        "* **numVotes** - número de avaliações do filme (`int`);\n",
        "* **actors_names** - atores/atrizes principais (`str`);\n",
        "* **directors_names** - diretores(as) do filme (`str`).\n",
        "\n",
        "**Observação:** esse conjunto de dados é uma versão transformada dos dados originais. Por exemplo, gêneros muito populares ou raros foram removidos.\n",
        "\n",
        "**ATENÇÃO**\n",
        "É possível que seu computador não consiga lidar com o conjunto de dados completo. Se for o caso, faça uma amostra aleatória de filmes do conjunto de dados. Deixe claro o processo de amostragem, o tamanho das amostras finais que considerou no trabalho e as especificações do computador utilizado."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os comandos abaixo mostram como os dados podem ser obtidos e carregados em um `DataFrame`."
      ],
      "metadata": {
        "id": "JUwkY_E-lD5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1AeYgV89TmYvNC__RDXr8hS0P6WOsChWg' -O filmes.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "z5tkfAI0OIrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bb7be6-a487-4a05-9233-e7482e06f9eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-17 01:38:31--  https://docs.google.com/uc?export=download&id=1AeYgV89TmYvNC__RDXr8hS0P6WOsChWg\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.64.100, 173.194.64.101, 173.194.64.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.64.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1AeYgV89TmYvNC__RDXr8hS0P6WOsChWg&export=download [following]\n",
            "--2025-02-17 01:38:31--  https://drive.usercontent.google.com/download?id=1AeYgV89TmYvNC__RDXr8hS0P6WOsChWg&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.126.132, 2607:f8b0:4001:c1d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.126.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11380332 (11M) [application/octet-stream]\n",
            "Saving to: ‘filmes.csv’\n",
            "\n",
            "filmes.csv          100%[===================>]  10.85M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-17 01:38:37 (98.3 MB/s) - ‘filmes.csv’ saved [11380332/11380332]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpZGHWs7p0C2"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"filmes.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "a = df['sinopse']"
      ],
      "metadata": {
        "id": "hrQlXDHxzl9r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "D = [\"É escória échange\", \"this is 1980  a another people and\", \"example example example\"]\n",
        "\n",
        "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-ZÀ-ÖØ-öø-ÿ]+\\b', stop_words='english')\n",
        "vectorizer.fit_transform(D)\n",
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "Xpx94zrcBg3T",
        "outputId": "2109cbad-c1a5-4194-c4af-57ee49d2d088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['escória', 'example', 'people', 'é', 'échange'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRqKIFG8oOjC"
      },
      "source": [
        "## Objetivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPvGwYn7oOjC"
      },
      "source": [
        "Vocês deverão utilizar aprendizado não supervisionado (neste caso, agrupamento) para investigar se há relação entre a **sinopse** de um filme e o(s) **gênero(s)** a que pertence.\n",
        "\n",
        "De forma mais específica, vocês deverão agrupar os filmes de acordo suas sinopses e, após isso, verificar a distribuição dos gêneros em cada grupo. Ou seja, se os filmes pertencentes ao mesmo grupo possuem o(s) mesmo(s) gênero(s) ou se as distribuições de gêneros de grupos diferentes são distintas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trabalho"
      ],
      "metadata": {
        "id": "HnmmtUKU6iy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de fazer o pré-processamento de dados a ideia é que vamos dividir a base de dados em pedaços menores mantendo a proporção da base de dados original ([StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html))."
      ],
      "metadata": {
        "id": "JrWBVR_nwBSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['genres'])\n",
        "y = df['genres']"
      ],
      "metadata": {
        "id": "6nCJt4sZ4gHN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Dividindo em 10 Folds estratificados\n",
        "skf = StratifiedKFold(n_splits=3)\n",
        "\n",
        "# Guardar todos os folds\n",
        "folds = list(skf.split(X, y))\n",
        "\n",
        "# Escolher um fold aleatório\n",
        "random_fold = np.random.RandomState(SEED).choice(len(folds))\n",
        "\n",
        "# Separar os dados do fold escolhido\n",
        "train_idx, test_idx = folds[random_fold]\n",
        "rand_X, rand_y = X.iloc[test_idx], y.iloc[test_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2kowOGD4p2i",
        "outputId": "04fbc751-456b-4e5c-ee4e-1e98526c9a23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando apenas a característica sinopse da amostra\n",
        "rand_X = rand_X['sinopse']"
      ],
      "metadata": {
        "id": "7fn4hVBa9lUv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando matriz de tokens para trabalhar com TF-IDF"
      ],
      "metadata": {
        "id": "EJiupFZI-VrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "\n",
        "# Baixando os modelos\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "GyAfNZiXf4Rp",
        "outputId": "4d27f3e1-7d6a-42f7-a5d1-8c5e24e7d24a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.7.1\n",
            "    Uninstalling en-core-web-sm-3.7.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.7.1\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "# Baixando as stop words do NLTK (apenas na primeira execução)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combina todas as stop words de todos os idiomas disponíveis no NLTK\n",
        "all_stop_words = set()\n",
        "for lang in stopwords.fileids():\n",
        "    all_stop_words.update(stopwords.words(lang))\n",
        "\n",
        "# Inicializando o PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Carregando modelos do SpaCy para diferentes idiomas\n",
        "spacy_models = {\n",
        "    'en': spacy.load('en_core_web_sm'),\n",
        "    'es': spacy.load('es_core_news_sm'),\n",
        "    'de': spacy.load('de_core_news_sm'),\n",
        "    'fr': spacy.load('fr_core_news_sm')\n",
        "}\n",
        "\n",
        "def tokenize_stem(text):\n",
        "    try:\n",
        "        # Detecta o idioma do texto\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = 'en'  # Padrão para inglês caso não detecte\n",
        "\n",
        "    # Usa o modelo SpaCy correspondente ao idioma\n",
        "    nlp = spacy_models.get(lang, spacy_models['en'])\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenização, Lematização e Remoção de Stop Words (tudo junto com SpaCy)\n",
        "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "    # Remove as stop words multilíngues\n",
        "    tokens = [word for word in tokens if word not in all_stop_words]\n",
        "\n",
        "    # Aplica stemming usando o PorterStemmer\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Criando o TfidfVectorizer com Stemming e Remoção de Números\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df = 0.001,\n",
        "    tokenizer=tokenize_stem,  # Usa a função personalizada\n",
        "    token_pattern=None  # Ignorado quando usamos tokenizer personalizado\n",
        ")\n",
        "\n",
        "# Ajusta o vectorizer no conjunto completo (ou apenas no treino, se preferir)\n",
        "vectorizer.fit(rand_X)\n",
        "\n",
        "# Transforma os dados de rand_X\n",
        "rand_X_tfidf = vectorizer.transform(rand_X)\n",
        "\n",
        "tfidf_dataframe = pd.DataFrame(rand_X_tfidf.todense(), columns = vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "AE8Df14NhHnB",
        "outputId": "59417673-7f81-4345-9f82-17f6352c64dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dataframe.info()"
      ],
      "metadata": {
        "id": "vESh_FkIiiI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = [\n",
        "#     \"The cat is sleeping on the sofa.\",\n",
        "#     \"A cat sleeps peacefully on the couch lvare\tçigdem\téchang\témigré\",\n",
        "#     \"Dogs are playing outside while the cat sleeps inside.\",\n",
        "#     \"The quick brown fox jumps over the lazy dog.\"\n",
        "# ]\n",
        "\n",
        "# # Inicializando o TfidfVectorizer com a função de pré-processamento personalizada\n",
        "# vectorizer = TfidfVectorizer(tokenizer=tokenize_stem, lowercase=False)\n",
        "# tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# # Exibindo as features (palavras relevantes) e a matriz TF-IDF\n",
        "# print(\"Features:\", vectorizer.get_feature_names_out())\n",
        "# print(\"\\nTF-IDF Matrix:\")\n",
        "# print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "id": "fHRsjRuod3UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dataframe.head()"
      ],
      "metadata": {
        "id": "gtyREcYLc_Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine\n",
        "import pandas as pd\n",
        "\n",
        "pca = PCA(n_components = 4000)\n",
        "pca.fit(tfidf_dataframe)\n",
        "\n",
        "#@title\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "plt.plot([i for i in range(1, pca.n_components + 1)], np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel(r'$k$ - Número de componentes principais')\n",
        "plt.ylabel(r'$f(k)$ - Fração cumulativa da variância explicada');"
      ],
      "metadata": {
        "id": "0vUwb8UrwAJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq1Vt0c9hoPa"
      },
      "source": [
        "## Metodologia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akm5BHH7wkKX"
      },
      "source": [
        "Para atingir o objetivo proposto, vocês deverão seguir ao menos os seguintes passos:\n",
        "1. Pré-processamento dos dados textuais;\n",
        "2. Construção da matriz de TF-IDF;\n",
        "3. Redução de dimensionalidade, via PCA (Leia também sobre o *Truncated SVD*: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html);\n",
        "4. Aplicação de ao menos dois algoritmos de agrupamento vistos na disciplina;\n",
        "5. Validação da metodologia utilizada;\n",
        "6. Interpretação dos resultados.\n",
        "\n",
        "**PS:** essas são as exigências mínimas. Caso vocês queiram fazer/propor algo extra, será permitido (e pode ajudar na nota se a proposta for relevante)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-yvMFikhq_X"
      },
      "source": [
        "## Resultados esperados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecXkqcTOzuTk"
      },
      "source": [
        "Dois tipo de resultados são esperados: validação e análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPMymRAQz6Og"
      },
      "source": [
        "### Resultados de validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR2SWoRYz-xS"
      },
      "source": [
        "O trabalho deve conter resultados mostrando que os algoritmos (e respectivos parâmetros) foram escolhidos e utilizados de forma correta. Entre os pontos importantes:\n",
        "1. Como foi escolhido o número de componentes principais para o PCA? O número de componentes principais tem um impacto significativo nos resultados? É necessário usar PCA neste trabalho?\n",
        "2. Como o número de grupos para cada algoritmo de agrupamento foi definido? A escolha do algoritmo de agrupamento impacta significativamente os resultados? E o número de grupos?\n",
        "\n",
        "Fará parte da avaliação a forma que escolherem para validar a metodologia. Vocês podem recorrer a visualizações interessantes, medidas internas..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEz5n3-50Lsx"
      },
      "source": [
        "### Análise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYodSngv0NZg"
      },
      "source": [
        "Após validar e entender os resultados (e se convencerem que estão corretos e fazem sentido), você deve responder as perguntas principais:\n",
        "- Há diferença nas distribuições de gêneros dos filmes em grupos diferentes?\n",
        "- Há relação entre as sinopses dos filmes e os respectivos gêneros?\n",
        "\n",
        "- **Opcional, pontuação extra:** se, além das sinopses, você também considerar as demais informações no conjunto de dados (avaliações, atores/atrizes, diretores(as), título, ano e duração) para os algoritmos de agrupamento, é possível encontrar uma melhor associação entre os grupos e os gêneros dos filmes?\n",
        "Novamente, recorra às ferramentas que já vimos no curso para responder essa pergunta: visualizações, medidas externas... **soluções que tiverem achados interessantes nessa parte do trabalho ganharão ponto extra**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ASld2-huE3"
      },
      "source": [
        "## Observações importantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UO36M8FpCRw"
      },
      "source": [
        "1. Durante a fase de agrupamento, apenas a informação da sinopse deve ser utilizada. A informação de **gênero dos filmes** deve ser usada apenas após a fase de agrupamento para validar e analisar os resultados.\n",
        "2. Vocês podem usar todas as bibliotecas Python que venho mostrando em aula. Caso queiram usar algo muito diferente (por exemplo, alguma biblioteca de uso comercial), perguntem ao professor antes (em nosso fórum do AVA).\n",
        "3. Lembrem-se que este trabalho vale 30% da nota do semestre. Vocês serão avaliados pela:\n",
        "  - Qualidade dos resultados quantitativos;\n",
        "  - Escolha e condução da metodologia;\n",
        "  - Uso de visualizações informativas e bem feitas;\n",
        "  - Explicação dos passos seguidos e das decisões tomadas;\n",
        "  - Justificativas para as decisões tomadas;\n",
        "  - Análise dos resultados;\n",
        "  - Conclusões.\n",
        "4. Qualquer colaboração, entre grupos ou de fontes externas, deve ser citada e mencionada no trabalho. **NÃO HAVERÁ TOLERÂNCIA PARA COLABORAÇÕES INDEVIDAS!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsoCRlShxue"
      },
      "source": [
        "## Grupos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y2N5Z6z5hg2"
      },
      "source": [
        "- **Graduação:** até 3 pessoas\n",
        "- **Pós-graduação:** individual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZB-sI2h5SD"
      },
      "source": [
        "## Entrega"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JoLe1135rh-"
      },
      "source": [
        "- **Linguagem de programação:** Python\n",
        "- **O que entregar:** um *Jupyter Notebook*, contendo texto, código e resultados. Deve ser possível que o professor execute o seu código, mas o arquivo ``.ipynb`` submetido já deve conter todos os resultados (i.e., deve ser possível corrigir seu trabalho apenas abrindo o arquivo). Apenas um integrante do grupo deve fazer a submissão no AVA. Lembrem-se de colocar a informação de todos os integrantes do grupo (nome, matrícula e e-mail)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euhJZNqK6jCI"
      },
      "source": [
        "## Dúvidas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKBfomkc6l8e"
      },
      "source": [
        "Postem em nosso fórum do AVA!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Lista de [stopwords](https://github.com/igorbrigadir/stopwords/blob/21fb2ef149216e3c8cac097975223604ae1e2310/en/rouge_155.txt) que encontramos na documentação do Scikitlearn\n",
        "-"
      ],
      "metadata": {
        "id": "EYs69FKxWiUy"
      }
    }
  ]
}